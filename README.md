## About study

<p align="justify">Software metrics provide means to quantify several attributes of software systems. The effective measurement is dependent of appropriate metric thresholds as they allow characterizing the quality of software systems. Indeed, thresholds have been used for detecting a variety software anomalies. Previous methods to derive metric thresholds do not take characteristics of software domains into account, such as the difference between size and complexity of systems from different domains. Instead, they rely on (generic) thresholds that are derived from heterogeneous systems. Although derivation of reliable thresholds has long been a concern, we also lack empirical evidence about threshold variation across distinct mobile software domains. This paper proposes a domain-sensitive method to derive thresholds that respects metric statistics and is based on benchmarks of systems from the same domain. To evaluate the proposed method, we manually mined one hundred mobile systems from GitHub, measured them using a set of seven well-known metrics, derived thresholds, and validated them through qualitative and quantitative analyses. As a result, we observed that software domain is a relevant factor to be considered when building benchmarks for threshold derivation.</p> 

**All data used in this study are available.**

Click here to see the raw data, which contains metrics of each software domains. <a href="https://github.com/sbcars18/sbcars18/tree/master/metrics/" target="_blank"><img src="https://cdn2.iconfinder.com/data/icons/snipicons/5000/download-alt-48.png" /></a>

<p align="justify">
  Download (Available Soon) <a href="Link" download="Download">   <img src="https://cdn2.iconfinder.com/data/icons/snipicons/5000/download-alt-48.png" /></a>
</p>
<p align="justify">
  To install: Paste the Jar file inside Dropins in Eclipse installation folder and Open Threshold Warning View.
</p>



